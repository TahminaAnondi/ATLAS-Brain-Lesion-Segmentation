{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (4.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from nibabel) (21.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nibabel) (62.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from nibabel) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->nibabel) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation, Dropout, SimpleRNN, LSTM,Bidirectional\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, SpatialDropout2D,Conv2DTranspose,Concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "def load_streamlines(dataPath, subject_ids, bundles, n_tracts_per_bundle):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(subject_ids)):\n",
    "        for c in range((len(bundles))):\n",
    "            filename = dataPath + subject_ids[i] + '/' + bundles[c] + '.trk' \n",
    "            tfile = nib.streamlines.load(filename)\n",
    "            streamlines = tfile.streamlines\n",
    "\n",
    "            n_tracts_total = len(streamlines)\n",
    "            ix_tracts = np.random.choice(range(n_tracts_total), n_tracts_per_bundle, replace=False)\n",
    "\n",
    "            streamlines_data = streamlines.get_data()\n",
    "            streamlines_offsets = streamlines._offsets\n",
    "            for j in range(n_tracts_per_bundle):\n",
    "                ix_j = ix_tracts[j]\n",
    "                offset_start = streamlines_offsets[ix_j] \n",
    "                if ix_j < (n_tracts_total - 1):\n",
    "                    offset_end = streamlines_offsets[ix_j + 1]\n",
    "                    streamline_j = streamlines_data[offset_start:offset_end] \n",
    "                else:\n",
    "                    streamline_j = streamlines_data[offset_start:] \n",
    "                X.append(np.asarray(streamline_j))\n",
    "                y.append(c)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = '/DL_course_data/Lab5/HCP_lab/'\n",
    "train_subjects_list = ['599469','599671','601127']\n",
    "val_subjects_list = ['613538']\n",
    "n_tracts_per_bundle = 20\n",
    "\n",
    "bundles_list = ['CST_left', 'CST_right']\n",
    "X_train, y_train = load_streamlines(dataPath, train_subjects_list, bundles_list, n_tracts_per_bundle)\n",
    "X_val, y_val = load_streamlines(dataPath, val_subjects_list, bundles_list, n_tracts_per_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(units):\n",
    "    model = Sequential()\n",
    "    # Add four LSTM layers\n",
    "    model.add(Bidirectional(LSTM(units, return_sequences=True, stateful = True),batch_input_shape=(1,None, 3)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=units, return_sequences= True, stateful = True ))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=units, return_sequences= True, stateful = True ))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(units=units, return_sequences= False, stateful = True ))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence \n",
    "class MyBatchGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size=1, shuffle=True):\n",
    "        self.X = X \n",
    "        self.y = y\n",
    "        self.batch_size = batch_size \n",
    "        self.shuffle = shuffle \n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        'Get number of batches per epoch'\n",
    "        return int(np.floor(len(self.y)/self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        return self.__data_generation(index)\n",
    "    def on_epoch_end(self):\n",
    "        'Shuffle indexes after each epoch' \n",
    "        self.indexes = np.arange(len(self.y)) \n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __data_generation(self, index):\n",
    "        Xb = np.empty((self.batch_size, *self.X[index].shape)) \n",
    "        yb = np.empty((self.batch_size, 1))\n",
    "        for s in range(0, self.batch_size):\n",
    "            Xb[s] = self.X[index]\n",
    "            yb[s] = self.y[index] \n",
    "        return Xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_10 (Bidirecti  (1, None, 20)            1120      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (1, None, 20)             0         \n",
      "                                                                 \n",
      " lstm_47 (LSTM)              (1, None, 10)             1240      \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (1, None, 10)             0         \n",
      "                                                                 \n",
      " lstm_48 (LSTM)              (1, None, 10)             840       \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (1, None, 10)             0         \n",
      "                                                                 \n",
      " lstm_49 (LSTM)              (1, 10)                   840       \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (1, 10)                   0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (1, 2)                    22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,062\n",
      "Trainable params: 4,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "LR = 1e-3\n",
    "model = model(10)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = LR), metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-735524486c38>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(MyBatchGenerator(X_train, y_train, batch_size=1),\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_13/bidirectional_10/forward_lstm_46/PartitionedCall]] [Op:__inference_train_function_45592]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/xinyi/Documents/GitHub/deep_learning_G5/Lab6/Lab_6_task_2.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xinyi/Documents/GitHub/deep_learning_G5/Lab6/Lab_6_task_2.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit_generator(MyBatchGenerator(X_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xinyi/Documents/GitHub/deep_learning_G5/Lab6/Lab_6_task_2.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49mn_epochs, validation_data\u001b[39m=\u001b[39;49mMyBatchGenerator(X_val, y_val, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xinyi/Documents/GitHub/deep_learning_G5/Lab6/Lab_6_task_2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 validation_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(X_val))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:2260\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[39m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m \n\u001b[1;32m   2251\u001b[0m \u001b[39mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2252\u001b[0m \u001b[39m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m \u001b[39m  this endpoint.\u001b[39;00m\n\u001b[1;32m   2254\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2256\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m`Model.fit_generator` is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2257\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwill be removed in a future version. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   2258\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   2259\u001b[0m     stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m-> 2260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   2261\u001b[0m     generator,\n\u001b[1;32m   2262\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   2263\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m   2264\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2265\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   2266\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m   2267\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   2268\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m   2269\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m   2270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2273\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   2274\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_13/bidirectional_10/forward_lstm_46/PartitionedCall]] [Op:__inference_train_function_45592]"
     ]
    }
   ],
   "source": [
    "model.fit_generator(MyBatchGenerator(X_train, y_train, batch_size=1), \n",
    "                epochs=n_epochs, validation_data=MyBatchGenerator(X_val, y_val, batch_size=1), \n",
    "                validation_steps=len(X_val))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
